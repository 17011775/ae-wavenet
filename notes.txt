There are three generators:

1. gen_path: generates the path and ID
2. _wav_gen: simple pipe for _gen_path
3. _gen_slice: (gen_function) 

How to restore state?


_wav_gen doesn't have any state
_gen_path can be easily restored, but not necessarily saved...


What signals the end of a training regime?  A keyboard interrupt, and
then a handler for that keyboard interrupt.  That means the iterators in use
will have a state and they can be stored.

If you re-raise it from each generator, does each one receive it appropriately?  Or,
only the ones that are currently executing?

For gen_slice, how do we 



The exception can happen at *any* moment - and, I'm not sure whether this means
any individual Python statement, or something even more granular.

Suppose you have some statement that affects the state of the generator.  For instance,
you regard the contents of wav and ids as the 'state' of gen_slice, together with the
state of wav_gen which it encloses.

There are three statements in the generator.  The first updates wav_gen.  The
second updates wav, and the third updates ids.  Now, the exception can occur at
any time, so it might occur in between the first and second, or second and third, or
after the third.  But, we can only re-start the generator at the beginning.  So, would
it be possible to write it in such a way that it can be restored to the same state?

In gen_slice, it doesn't appear so.  It appears that  

It is interesting that, perhaps during the creation of the closure, we also give it
state in the things it encloses?  It could be that just after the yield is the best place
to record state.  

Think of it like this:  Suppose you had a single generator that generated the counting
numbers.  You run it for awhile, and then interrupt it.  Then, you record the last
value that it yielded.  The restore routine could just be to iterate it until it
yielded the same value.  The only problem is this is wasteful - you are repeating some
work already done.  However, if you only repeat a little bit of the work, it's okay.

Unfortunately, there isn't a way to rewind an iterator, even by one position.

Let's say you are interrupted at line 130 in data.py.  You've already consumed the
next wav data from the generator.  In order to restore that, you do the following:

Another problem is that the initial construction of the B slice_gen generators
doesn't actually draw from wav_gen.  So, if we were to re-execute the _gen_slice_batch
function, it would have no way to restore 

Perhaps adopt the following principles in constructing a generator.

For one thing, running the generator is the easiest way to change its state - after
all, that is what it is designed for.  We just need a virtual measure for position,

So, what we would like is to be able to have a virtual 'current' position.  When
interrupted, we pickle just the current logical position.  To restore, we read the
pickled current position and instantiate the generators to that position.

How can this be done?

It is likely not possible to recover the exact order in which each wav file is loaded
into each channel of the batch.  But, the only thing it should affect is the order in
which gradients are summed.  (Gradient terms never interact across items of a batch,
and they are summed at the end)

So, here is a proposed protocol for restoring:

1. Retrieve the saved wav_gen current position and instantiate wav_gen generator
   to one position previous to it.

2. Retrieve each of the B sub-positions of the gen_slice generators.  Instantiate each
   of these to this position (not the previous) 


To write the wav_gen generator to restore to a particular position, we use the
list iterator.  Actually, I'd like to avoid using pickle.dump due to API compatibility
issues.  A better way would be to just store simple text integers.

Actually, also, since all slice_gen's share the same wav_gen, we need to find the current
position of wav_gen and rewind B positions.  Then, there needs to be a repeatable sequence
of random numbers as well, gah!

But, that is fine.  We just record the following:

1. random_seed
2. last epoch E
3. path_gen position P
4. B slice_gen positions

Or, maybe it's best to just maintain a single E and P for the minimum.  But, this would mean
maintaining P and E positions for all B batch channels, because we don't know what the next
minimum will be.

So one way we could achieve this is to return the epoch, file_index, slice_index.  or,
we could instead maintain these as properties of the slice_gens.  it seems a bit gauche
to store these properties on the functors themselves.  but is there a better way?

An advantage to function properties is that they are named, and can be accessed at leisure,
rather than needing to be collected at every call.

So, if I have an interrupt handler, how can I guarantee that the various state variable
representatives are accurate?

Let's see...at any moment, there can be a keyboard interrupt.  The main thing that's important
is that the updating of the weights and of the variables describing state of the data
reader are in sync.  But, obviously, there is no way to make these updates atomic, is there?


So, from a high-level point of view, the training will involve two successive changes
in state.

1. data position
2. model parameters

So, there are two questions.  These most likely will update in alternating fashion.
The question is whether there is, or should be any space in between the two.

D  M  D  M  D  M ...

What is actually happening?

1. model initialized randomly
2. data initialized to starting position

Loop:
3. compute batch of data 
4. compute gradients
5. update model
6. advance to next data position (and load data))

So, if you checkpointed between 5 and 6, then you would end up re-processing on
an already-processed slice of data.  So, really, steps 5 and 6 should be uninterruptible.

Then, after that, the signal handler should just write the joint data/model state to disk.
But, the key is to make those updates atomic.

I think the problem is solved then - write all the update code in a single thread so that
the collection of variables that are updated in that thread are always in a consistent
state before and after it executes.  When a KeyboardInterrupt comes in, it won't interrupt
the execution of that thread, and when the handler gets invoked, it will be guaranteed
to have those variables in a consistent state for writing to disk.

It doesn't actually matter *where* in the different structures these variables are.
All that is needed is the update thread can access them.  But, given their logical
relation, it might be nice to collect them in a convenient place.

For MaskedSliceWav: 

save / load:
np.random.RandomState
epoch (earliest among the batch channels)
file_index (earliest among the batch channels)
slice_indices (in order of (epoch, file_index))

path_gen should yield:
epoch
file_index



Are we also going to save each slice index position?  If so, we need to
save each one in association with the file it is part of, I think.

And, that also means saving the epoch for each file index.  The easiest way is
probably to save the first epoch and file index, and then save the list of
slice_indices sorted in order of (epoch, file_index).

Then, during restoration, each new instantiation of slice_gen will pull the next
file, and advance to the appropriate slice

I think the best thing is for each slice_gen object to record its own state, which
will need to be the associated epoch, file_index and slice_index.

It might be best to write slice_gen in a way that it can be

Each slice_gen just needs to know what index it is, and then it can
access the class instance variables telling it where to fast-forward.

At what moment do we consider the slice_index to be "current"?

We will have a separate thread that runs the entire syncing routine.  It will
have to read 

epoch  file_index  slice_index

Now all we need is to translate 'position' into its proper summary statistic.  This
just requires sorting it by (epoch, file_index), recording the earliest epoch and file_index,
and recording the order of slice_index field.

But, when should this summary occur?  Just after the final yield, we know that the
previous position must have been processed.

In fact, it's inappropriate for _gen_slice_batch to yield position, since this information
is not useful for the model.  But, it is at least tightly coupled to the yielded wav and
ids data.  If we set the state of MaskedSliceWav object to that position just before
yielding, then, at the moment when we want to freeze the total state, when should we do that?


G: yield (t)
M: compute gradients (t)
M: update weights and data position (t)
M: next(G)
G: compute new data (t+1)
G: yield (t+1)
M: compute gradients (t+1)
M: update weights and data position (t+1)
M: next(G)

Note that the process of *updating* the data/model checkpointable state is what needs
to be uninterruptible.

What's the best way to do this?  We need MaskedSlice 

The checkpointing logic should not be stored in data module, because it must be done
in coordination with the model.  So, details like the interval, or naming scheme
need to be handled separately.

In fact, even saving and restoring shouldn't be handled here, because it must be done
in coordination with the model.  Instead, the data module should just provide
a single object (called IterPosition) that encapsulates its state that needs to
be serialized.  And, MaskedSliceWav should provide a public member function that
allows a client to update its state.  It is the client's responsibility to perform
these updates in uninterruptible fashion using a separate thread.

Is it perhaps better to create another class that makes the

Now, another wrinkle.  batch_gen is just a function.  it must be called first in order
to return a  

Can you pass a dict that has extra keys?  No.  It's not possible to do that.

You could instead have a special class that represents all the hyperparams of WaveNet?

Then, you could just use **WaveNetHyper.__dict__ as the arguments to WaveNet

There doesn't seem any way to have the best of both worlds in writing a class __init__
method.  If you want the __init__ method to be self-documenting, i.e. to have individual
arguments (as opposed to just one argument which is a dict), then you need to iterate
through them.

If you do define the WaveNetHyper, then initializing it will also require the same sort of
work.   Another option is to group arguments logically.  But, it still requires constructing
other structures and initializing them.  There doesn't seem to be any point to doing that.

So, as far as the argument parsing in the main program, the best would be



n_batch, n_win, n_in, n_kern, n_lc_in, n_lc_out, lc_upsample_strides, lc_upsample_kern_sizes, n_res, n_dil, n_skp, n_post, n_quant, n_blocks, n_block_layers, jitter_prob, n_speakers, n_global_embed, bias

What's the deal with Jitter?  WaveNet jitters the local conditioning signal.  But, note
that the LC signal is one-to-one with 



Get RF size for a single phoneme embedding.  This will be a function of:

1) window size for MFCC calculation
2) stride of the MFCC calculations (160)
3) filter sizes of each convolution
4) strides of each of these filters


Get RF size for a single local conditioning vector (upsampled from the phoneme embeddings)

This will be a function of:

1) sizes and inverse strides of each transpose convolution.

Besides that, we need to decide the phase of each of these operations.  Do we regard
the output of one of these transpose convolutions as aligned to the left, the right,
or the center of its receptive field?  To me, it makes the most intuitive sense to
define it as centered.


Conv4:                                                *
Conv3:
Conv2:
Conv1:          *************************************************************
MFCC:   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *


Cumulative Inverse Stride is just the product of stacked inverse strides.
Cumulative Receptive Field size is R1 + R2 - 1


Level              1    2    3   4
InvStride          5    4    4   4
FilterElems        5    5    5   5
FilterSpan        21   17   17  17
FilterSpanCumul   21   37   53  69
InvStrideCumul     5   20   80 320
CumulInputs        5   

One useful way to look at it is to list the absolute positions of the first and last
elements (whether they are real inputs or zero paddings) at each level:






3.          960  1024
2.      800     1120
1.     1 1600  320  1920    


Do it inductively, starting with position p, and working downward.

One can recast the stacked transpose convolution architecture as a static, repeating
architecture with the same filter placed at every position (at the time resolution t)

Actually, no, that is not correct.  You would need to connect it up in the way


So, does it make sense to design the decoder (WaveNet) to use a centered context to compute
local conditioning vectors, but to not have the encoder use a centered context?  

It's an odd thing - I can't really see what principle is operating here.  We could look
at the following:

1. Take the receptive field of WaveNet for generating just the next time step.  This is the
positions [t-2048, t-1].  Now, look at the receptive field in terms of the embedding vectors
to generate the local conditioning vectors in that window.  It'll actually be:

[t-2856, t+808]

Now, take the receptive field of the actual wav file used to generate the embedding vectors in
that window.  This will now be something even wider, such as:

[t-3000, t+1000]

Should it bother us that there is a different context being used to generate the next timestep
as the one used for the inputs for that generation?

Well, it doesn't break any autoregressive factorization, because WaveNet doesn't need to compute
embedding vectors.  But, we *do* want the embedding vectors to be trained in such a way that the
input used roughly corresponds to the output produced from each one.  The problem is that, while
there is a definite stretch of input wav form that is used to produce each embedding vector, there
isn't any real way to assign a "definite" stretch of wav that is being generated from
the 


Training regimen

Note that the encoder produces outputs at 50 Hz (for a 16000 Hz input), thus it
consumes windows of input skipping 320 positions each time.  The decoder
consumes windows at 16000 Hz.  So, one inference event from the encoder is used
as input for 320 inference events for the decoder.  This seems to imply (I
could maybe check with Jan) that the encoder should receive gradients averaged
over all 320 events of the decoder (WaveNet).

But, this situation is really no different than any many-to-one architecture,
such as convolutions.  Gradients are averaged in this case.

Now, how should we deal with the following issues in the context of
concatenated, sliced inputs.  As a refresher, this means:  whole wav files are
parsed and virtually concatenated, and from this concatenation, we take
individual slices.  Each slice is meant as a collection of overlapping windows,
where each window is a single training example.  Note that now, a "batch" of
samples has two dimensions, which I denote (n_batch, n_win).  n_batch is the
number of wav files being processed in parallel.  n_win is the number of
consecutive, overlapping windows being processed at a time.

1. logically invalid windows.  Any logical window that spans a boundary between
two different wav files is logically invalid.

2. need to trim output of librosa's mfcc function, since it automatically does
padding.

3. how to coordinate the encoder window and decoder window correspondence?  In
this case, the problem is solved by requiring the second batch dimension, n_win,
to be a multiple of 320.

There seems to be another weird wrinkle to this.  Suppose the wavenet decoder
architecture dictates that we need 20 embedding vectors to produce a set of
2048 local conditioning vectors needed to do a single inference.  Now, thinking
in groups of 320, we could naturally get 2368 (2048 + 320) local conditioning
vectors, since this is inherent in the WaveNet model's transpose convolution
upsampling module.  Thus, from the perspective of the encoder, it has produced
20 independent inferences, each from an input window.  This set of 20 different
inferences will be used in the 320 inferences of WaveNet, so we will naturally
average the gradients.

However, when we move over one encoder input window position (320 timesteps), we
will be repeating 19 of these inferences, and each of those will again be exposed
to an additional 320 WaveNet inferences, with their averaged gradients.  So,
overall, each encoder inference will be used in 6400 output inferences.  Is it
necessary to average these all at once?

I don't see why.  After all, note that the minibatch gradient calculation is
just an approximation anyway.  And, we just as well repeat the same data some
times.

So, I *think* that, if we follow the rule that we feed the system overlapping
windows, and we calculate the encoder input overlap so that the successive
decoder inputs are non-repeating and complete (i.e. first batch is inputs
1-5000, second batch is inputs 5001-10000), then each encoder input training
example will be exposed to the same number (i.e. 6400) of decoder inferences.
So, for instance, in many of these, all 6400 will be processed at once.  In the
examples towards the end of a batch, we would have a gradually diminishing
number of them, diminishing by 320 each time. (i.e. 6400, 6080, 5760, ..., 320)
and towards the beginning of a batch, also gradually decreasing in the same
manner.

So, the key design principle here is to

1. make the batch n_win size a multiple of 320
2. make consecutive windows overlap by one step (320 time steps) less than
   the receptive field of a single 

There are a few remaining issues.

1. Given the fact that the autoencoder requires two different windows (possibly
   one containing the other) of wav input, one or both may span a boundary and 
   thus be invalid.  So, we want a mask to be provided to deactivate gradient
   tallying for these windows.

But, we need a coordinate system for this mask.  The mask should have
one element per logical sample.  So, its dimension should be (B, N), and the
dimensions of the two input wav windows should be:

(B, N + hybrid RF)
(B, N + decoder RF)

where it is understood that "hybrid RF" is the RF induced by a single local
conditioning vector, first back onto the encoder output (via transposed
convolution), and then from encoder output to encoder input.

The output tensor of WaveNet (and thus the autoencoder) will be the same dimension as
the number of samples (B, N) and thus of the mask.  So, naturally, the coordinates
of the mask will correspond to each output element.  There will be no issue in
applying the mask just before the loss function.

In short, each element of the mask should be true or false depending on whether the
joint receptive fields giving rise to the single element prediction at that position
are both valid or not.  To calculate this, we take:

1. the decoder receptive field sized window of wav input to the left of the predicted element.
2. the joint receptive field of the same-sized window of local conditioning vectors
   in terms of encoding vectors (which is determined by WaveNet's transpose convolution
   module.
3. the joint receptive field of the encoder corresponding to the set of encoding vectors.

To get a picture of what this looks like, note that, with a single receptive field, the
window of invalid positions around a boundary is twice the width of the receptive field,
and it is suitably offset by whatever the relation is between the position of the predicted
element and its receptive field.  So, in the case of WaveNet alone, if we have a single
input Wav file, it has two boundaries (it's start and end).  The invalid windows are
the size of the receptive field on either end.  

Instead, let's make the mask the same size as the wav input.  So, we don't have to worry
about an "offset".  Then, the mask just has a pretty simple pattern - we start with all true,
then we superimpose the field of influence of each boundary on the mask.  In the case of a 
single input / single receptive field, this is a single field of influence.  In the present
case, we have two inputs, each with a different sized receptive field.  The field of influence
is the same size as the receptive field, and the offset between the position of the boundary
in the input and the start position of the field of influence in the mask is the same as
the offset between the start of the receptive field and the predicted element.

One complicating factor for this particular case is that, unlike with simple WaveNet, this
offset is not a constant in the autoencoder, due to the difference in stride .

So, what do we need to implement the mask properly?  We just need to be able to calculate
four numbers - the start and end points of each receptive field, in the coordinates of
the input wav file.  Assuming the two intervals overlap, (which I believe they do) we then
take the union of the interval, and compute the offset relative to the position in the
mask.  If any boundary resides within these two offsets, we mark the mask position as invalid.


Scrapped the idea of masks.  Instead, we just use a buffered data provider that gives
100% valid sample windows, batched by (B, N).  Depending on the user's choice of a memory
buffer, it pre-reads wav file content, then exhaustively iterates through all possible
permutations of it before moving onto the next content.  One can also adjust this to
go through only some fraction of it, so as to increase mixing with the rest of the 
data set.

In any case, here's the outline:


