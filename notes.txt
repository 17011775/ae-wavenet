There are three generators:

1. gen_path: generates the path and ID
2. _wav_gen: simple pipe for _gen_path
3. _gen_slice: (gen_function) 

How to restore state?


_wav_gen doesn't have any state
_gen_path can be easily restored, but not necessarily saved...


What signals the end of a training regime?  A keyboard interrupt, and
then a handler for that keyboard interrupt.  That means the iterators in use
will have a state and they can be stored.

If you re-raise it from each generator, does each one receive it appropriately?  Or,
only the ones that are currently executing?

For gen_slice, how do we 



The exception can happen at *any* moment - and, I'm not sure whether this means
any individual Python statement, or something even more granular.

Suppose you have some statement that affects the state of the generator.  For instance,
you regard the contents of wav and ids as the 'state' of gen_slice, together with the
state of wav_gen which it encloses.

There are three statements in the generator.  The first updates wav_gen.  The
second updates wav, and the third updates ids.  Now, the exception can occur at
any time, so it might occur in between the first and second, or second and third, or
after the third.  But, we can only re-start the generator at the beginning.  So, would
it be possible to write it in such a way that it can be restored to the same state?

In gen_slice, it doesn't appear so.  It appears that  

It is interesting that, perhaps during the creation of the closure, we also give it
state in the things it encloses?  It could be that just after the yield is the best place
to record state.  

Think of it like this:  Suppose you had a single generator that generated the counting
numbers.  You run it for awhile, and then interrupt it.  Then, you record the last
value that it yielded.  The restore routine could just be to iterate it until it
yielded the same value.  The only problem is this is wasteful - you are repeating some
work already done.  However, if you only repeat a little bit of the work, it's okay.

Unfortunately, there isn't a way to rewind an iterator, even by one position.

Let's say you are interrupted at line 130 in data.py.  You've already consumed the
next wav data from the generator.  In order to restore that, you do the following:

Another problem is that the initial construction of the B slice_gen generators
doesn't actually draw from wav_gen.  So, if we were to re-execute the _gen_slice_batch
function, it would have no way to restore 

Perhaps adopt the following principles in constructing a generator.

For one thing, running the generator is the easiest way to change its state - after
all, that is what it is designed for.  We just need a virtual measure for position,

So, what we would like is to be able to have a virtual 'current' position.  When
interrupted, we pickle just the current logical position.  To restore, we read the
pickled current position and instantiate the generators to that position.

How can this be done?

It is likely not possible to recover the exact order in which each wav file is loaded
into each channel of the batch.  But, the only thing it should affect is the order in
which gradients are summed.  (Gradient terms never interact across items of a batch,
and they are summed at the end)

So, here is a proposed protocol for restoring:

1. Retrieve the saved wav_gen current position and instantiate wav_gen generator
   to one position previous to it.

2. Retrieve each of the B sub-positions of the gen_slice generators.  Instantiate each
   of these to this position (not the previous) 


To write the wav_gen generator to restore to a particular position, we use the
list iterator.  Actually, I'd like to avoid using pickle.dump due to API compatibility
issues.  A better way would be to just store simple text integers.

Actually, also, since all slice_gen's share the same wav_gen, we need to find the current
position of wav_gen and rewind B positions.  Then, there needs to be a repeatable sequence
of random numbers as well, gah!

But, that is fine.  We just record the following:

1. random_seed
2. last epoch E
3. path_gen position P
4. B slice_gen positions

Or, maybe it's best to just maintain a single E and P for the minimum.  But, this would mean
maintaining P and E positions for all B batch channels, because we don't know what the next
minimum will be.

So one way we could achieve this is to return the epoch, file_index, slice_index.  or,
we could instead maintain these as properties of the slice_gens.  it seems a bit gauche
to store these properties on the functors themselves.  but is there a better way?

An advantage to function properties is that they are named, and can be accessed at leisure,
rather than needing to be collected at every call.

So, if I have an interrupt handler, how can I guarantee that the various state variable
representatives are accurate?

Let's see...at any moment, there can be a keyboard interrupt.  The main thing that's important
is that the updating of the weights and of the variables describing state of the data
reader are in sync.  But, obviously, there is no way to make these updates atomic, is there?


So, from a high-level point of view, the training will involve two successive changes
in state.

1. data position
2. model parameters

So, there are two questions.  These most likely will update in alternating fashion.
The question is whether there is, or should be any space in between the two.

D  M  D  M  D  M ...

What is actually happening?

1. model initialized randomly
2. data initialized to starting position

Loop:
3. compute batch of data 
4. compute gradients
5. update model
6. advance to next data position (and load data))

So, if you checkpointed between 5 and 6, then you would end up re-processing on
an already-processed slice of data.  So, really, steps 5 and 6 should be uninterruptible.

Then, after that, the signal handler should just write the joint data/model state to disk.
But, the key is to make those updates atomic.

I think the problem is solved then - write all the update code in a single thread so that
the collection of variables that are updated in that thread are always in a consistent
state before and after it executes.  When a KeyboardInterrupt comes in, it won't interrupt
the execution of that thread, and when the handler gets invoked, it will be guaranteed
to have those variables in a consistent state for writing to disk.

It doesn't actually matter *where* in the different structures these variables are.
All that is needed is the update thread can access them.  But, given their logical
relation, it might be nice to collect them in a convenient place.

For MaskedSliceWav: 

save / load:
np.random.RandomState
epoch (earliest among the batch channels)
file_index (earliest among the batch channels)
slice_indices (in order of (epoch, file_index))

path_gen should yield:
epoch
file_index



Are we also going to save each slice index position?  If so, we need to
save each one in association with the file it is part of, I think.

And, that also means saving the epoch for each file index.  The easiest way is
probably to save the first epoch and file index, and then save the list of
slice_indices sorted in order of (epoch, file_index).

Then, during restoration, each new instantiation of slice_gen will pull the next
file, and advance to the appropriate slice

I think the best thing is for each slice_gen object to record its own state, which
will need to be the associated epoch, file_index and slice_index.

It might be best to write slice_gen in a way that it can be

Each slice_gen just needs to know what index it is, and then it can
access the class instance variables telling it where to fast-forward.

At what moment do we consider the slice_index to be "current"?

We will have a separate thread that runs the entire syncing routine.  It will
have to read 

epoch  file_index  slice_index

Now all we need is to translate 'position' into its proper summary statistic.  This
just requires sorting it by (epoch, file_index), recording the earliest epoch and file_index,
and recording the order of slice_index field.

But, when should this summary occur?  Just after the final yield, we know that the
previous position must have been processed.

In fact, it's inappropriate for _gen_slice_batch to yield position, since this information
is not useful for the model.  But, it is at least tightly coupled to the yielded wav and
ids data.  If we set the state of MaskedSliceWav object to that position just before
yielding, then, at the moment when we want to freeze the total state, when should we do that?


G: yield (t)
M: compute gradients (t)
M: update weights and data position (t)
M: next(G)
G: compute new data (t+1)
G: yield (t+1)
M: compute gradients (t+1)
M: update weights and data position (t+1)
M: next(G)

Note that the process of *updating* the data/model checkpointable state is what needs
to be uninterruptible.

What's the best way to do this?  We need MaskedSlice 

The checkpointing logic should not be stored in data module, because it must be done
in coordination with the model.  So, details like the interval, or naming scheme
need to be handled separately.

In fact, even saving and restoring shouldn't be handled here, because it must be done
in coordination with the model.  Instead, the data module should just provide
a single object (called IterPosition) that encapsulates its state that needs to
be serialized.  And, MaskedSliceWav should provide a public member function that
allows a client to update its state.  It is the client's responsibility to perform
these updates in uninterruptible fashion using a separate thread.

Is it perhaps better to create another class that makes the

Now, another wrinkle.  batch_gen is just a function.  it must be called first in order
to return a  

Can you pass a dict that has extra keys?  No.  It's not possible to do that.

You could instead have a special class that represents all the hyperparams of WaveNet?

Then, you could just use **WaveNetHyper.__dict__ as the arguments to WaveNet

There doesn't seem any way to have the best of both worlds in writing a class __init__
method.  If you want the __init__ method to be self-documenting, i.e. to have individual
arguments (as opposed to just one argument which is a dict), then you need to iterate
through them.

If you do define the WaveNetHyper, then initializing it will also require the same sort of
work.   Another option is to group arguments logically.  But, it still requires constructing
other structures and initializing them.  There doesn't seem to be any point to doing that.

So, as far as the argument parsing in the main program, the best would be



n_batch, n_win, n_in, n_kern, n_lc_in, n_lc_out, lc_upsample_strides, lc_upsample_kern_sizes, n_res, n_dil, n_skp, n_post, n_quant, n_blocks, n_block_layers, jitter_prob, n_speakers, n_global_embed, bias

What's the deal with Jitter?  WaveNet jitters the local conditioning signal.  But, note
that the LC signal is one-to-one with 



Get RF size for a single phoneme embedding.  This will be a function of:

1) window size for MFCC calculation
2) stride of the MFCC calculations (160)
3) filter sizes of each convolution
4) strides of each of these filters


Get RF size for a single local conditioning vector (upsampled from the phoneme embeddings)

This will be a function of:

1) sizes and inverse strides of each transpose convolution.

Besides that, we need to decide the phase of each of these operations.  Do we regard
the output of one of these transpose convolutions as aligned to the left, the right,
or the center of its receptive field?  To me, it makes the most intuitive sense to
define it as centered.


Conv4:                                                *
Conv3:
Conv2:
Conv1:          *************************************************************
MFCC:   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *


Cumulative Inverse Stride is just the product of stacked inverse strides.
Cumulative Receptive Field size is R1 + R2 - 1


Level              1    2    3   4
InvStride          5    4    4   4
FilterElems        5    5    5   5
FilterSpan        21   17   17  17
FilterSpanCumul   21   37   53  69
InvStrideCumul     5   20   80 320
CumulInputs        5   

One useful way to look at it is to list the absolute positions of the first and last
elements (whether they are real inputs or zero paddings) at each level:






3.          960  1024
2.      800     1120
1.     1 1600  320  1920    


Do it inductively, starting with position p, and working downward.

One can recast the stacked transpose convolution architecture as a static, repeating
architecture with the same filter placed at every position (at the time resolution t)

Actually, no, that is not correct.  You would need to connect it up in the way


So, does it make sense to design the decoder (WaveNet) to use a centered context to compute
local conditioning vectors, but to not have the encoder use a centered context?  

It's an odd thing - I can't really see what principle is operating here.  We could look
at the following:

1. Take the receptive field of WaveNet for generating just the next time step.  This is the
positions [t-2048, t-1].  Now, look at the receptive field in terms of the embedding vectors
to generate the local conditioning vectors in that window.  It'll actually be:

[t-2856, t+808]

Now, take the receptive field of the actual wav file used to generate the embedding vectors in
that window.  This will now be something even wider, such as:

[t-3000, t+1000]

Should it bother us that there is a different context being used to generate the next timestep
as the one used for the inputs for that generation?

Well, it doesn't break any autoregressive factorization, because WaveNet doesn't need to compute
embedding vectors.  But, we *do* want the embedding vectors to be trained in such a way that the
input used roughly corresponds to the output produced from each one.  The problem is that, while
there is a definite stretch of input wav form that is used to produce each embedding vector, there
isn't any real way to assign a "definite" stretch of wav that is being generated from
the 


Training regimen

Note that the encoder produces outputs at 50 Hz (for a 16000 Hz input), thus it
consumes windows of input skipping 320 positions each time.  The decoder
consumes windows at 16000 Hz.  So, one inference event from the encoder is used
as input for 320 inference events for the decoder.  This seems to imply (I
could maybe check with Jan) that the encoder should receive gradients averaged
over all 320 events of the decoder (WaveNet).

But, this situation is really no different than any many-to-one architecture,
such as convolutions.  Gradients are averaged in this case.

Now, how should we deal with the following issues in the context of
concatenated, sliced inputs.  As a refresher, this means:  whole wav files are
parsed and virtually concatenated, and from this concatenation, we take
individual slices.  Each slice is meant as a collection of overlapping windows,
where each window is a single training example.  Note that now, a "batch" of
samples has two dimensions, which I denote (n_batch, n_win).  n_batch is the
number of wav files being processed in parallel.  n_win is the number of
consecutive, overlapping windows being processed at a time.

1. logically invalid windows.  Any logical window that spans a boundary between
two different wav files is logically invalid.

2. need to trim output of librosa's mfcc function, since it automatically does
padding.

3. how to coordinate the encoder window and decoder window correspondence?  In
this case, the problem is solved by requiring the second batch dimension, n_win,
to be a multiple of 320.

There seems to be another weird wrinkle to this.  Suppose the wavenet decoder
architecture dictates that we need 20 embedding vectors to produce a set of
2048 local conditioning vectors needed to do a single inference.  Now, thinking
in groups of 320, we could naturally get 2368 (2048 + 320) local conditioning
vectors, since this is inherent in the WaveNet model's transpose convolution
upsampling module.  Thus, from the perspective of the encoder, it has produced
20 independent inferences, each from an input window.  This set of 20 different
inferences will be used in the 320 inferences of WaveNet, so we will naturally
average the gradients.

However, when we move over one encoder input window position (320 timesteps), we
will be repeating 19 of these inferences, and each of those will again be exposed
to an additional 320 WaveNet inferences, with their averaged gradients.  So,
overall, each encoder inference will be used in 6400 output inferences.  Is it
necessary to average these all at once?

I don't see why.  After all, note that the minibatch gradient calculation is
just an approximation anyway.  And, we just as well repeat the same data some
times.

So, I *think* that, if we follow the rule that we feed the system overlapping
windows, and we calculate the encoder input overlap so that the successive
decoder inputs are non-repeating and complete (i.e. first batch is inputs
1-5000, second batch is inputs 5001-10000), then each encoder input training
example will be exposed to the same number (i.e. 6400) of decoder inferences.
So, for instance, in many of these, all 6400 will be processed at once.  In the
examples towards the end of a batch, we would have a gradually diminishing
number of them, diminishing by 320 each time. (i.e. 6400, 6080, 5760, ..., 320)
and towards the beginning of a batch, also gradually decreasing in the same
manner.

So, the key design principle here is to

1. make the batch n_win size a multiple of 320
2. make consecutive windows overlap by one step (320 time steps) less than
   the receptive field of a single 

There are a few remaining issues.

1. Given the fact that the autoencoder requires two different windows (possibly
   one containing the other) of wav input, one or both may span a boundary and 
   thus be invalid.  So, we want a mask to be provided to deactivate gradient
   tallying for these windows.

But, we need a coordinate system for this mask.  The mask should have
one element per logical sample.  So, its dimension should be (B, N), and the
dimensions of the two input wav windows should be:

(B, N + hybrid RF)
(B, N + decoder RF)

where it is understood that "hybrid RF" is the RF induced by a single local
conditioning vector, first back onto the encoder output (via transposed
convolution), and then from encoder output to encoder input.

The output tensor of WaveNet (and thus the autoencoder) will be the same dimension as
the number of samples (B, N) and thus of the mask.  So, naturally, the coordinates
of the mask will correspond to each output element.  There will be no issue in
applying the mask just before the loss function.

In short, each element of the mask should be true or false depending on whether the
joint receptive fields giving rise to the single element prediction at that position
are both valid or not.  To calculate this, we take:

1. the decoder receptive field sized window of wav input to the left of the predicted element.
2. the joint receptive field of the same-sized window of local conditioning vectors
   in terms of encoding vectors (which is determined by WaveNet's transpose convolution
   module.
3. the joint receptive field of the encoder corresponding to the set of encoding vectors.

To get a picture of what this looks like, note that, with a single receptive field, the
window of invalid positions around a boundary is twice the width of the receptive field,
and it is suitably offset by whatever the relation is between the position of the predicted
element and its receptive field.  So, in the case of WaveNet alone, if we have a single
input Wav file, it has two boundaries (it's start and end).  The invalid windows are
the size of the receptive field on either end.  

Instead, let's make the mask the same size as the wav input.  So, we don't have to worry
about an "offset".  Then, the mask just has a pretty simple pattern - we start with all true,
then we superimpose the field of influence of each boundary on the mask.  In the case of a 
single input / single receptive field, this is a single field of influence.  In the present
case, we have two inputs, each with a different sized receptive field.  The field of influence
is the same size as the receptive field, and the offset between the position of the boundary
in the input and the start position of the field of influence in the mask is the same as
the offset between the start of the receptive field and the predicted element.

One complicating factor for this particular case is that, unlike with simple WaveNet, this
offset is not a constant in the autoencoder, due to the difference in stride .

So, what do we need to implement the mask properly?  We just need to be able to calculate
four numbers - the start and end points of each receptive field, in the coordinates of
the input wav file.  Assuming the two intervals overlap, (which I believe they do) we then
take the union of the interval, and compute the offset relative to the position in the
mask.  If any boundary resides within these two offsets, we mark the mask position as invalid.


Scrapped the idea of masks.  Instead, we just use a buffered data provider that gives
100% valid sample windows, batched by (B, N).  Depending on the user's choice of a memory
buffer, it pre-reads wav file content, then exhaustively iterates through all possible
permutations of it before moving onto the next content.  One can also adjust this to
go through only some fraction of it, so as to increase mixing with the rest of the 
data set.

In any case, here's the outline:


What would be a good unit test for checkpointing?

Problem: without using masks, and using slices of wav files that are consecutive overlapping
windows of samples, there is no way to ensure sampling evenly from each wav file, unless
the wav file is of length n * n_win + rf_sz.

But, since using a mask is prohibitively complicated, and using individual samples is
prohibitively inefficient, we instead just try to minimize the unevenness by choosing
a modest value for n_win.  This way, the wav files are close to some multiple of n_win,
and we still get some degree of efficiency but without compromising much on the 
evenness of the distribution.

In order to implement this, we will need just one more level of processing.  We start
with the current setup, considering all slice start positions from [0, n_items) (where n_items
is a prime number).

However, to really make use of this, we instead need to generate the x % n_win == offset ones.
How many?  We ask the user for a memory estimate, then we divide by n_win, and choose the
greatest lower bound prime number, then generate random permutations from [0, glb).  For each
drawing d, we take n_win * d + off, and use that as the logical start position for a slice.

Upon the next buffer reloading (instantiation of slice_gen), we choose a different offset
based on random state.

This whole thing will take only five or six lines of additional code.

import json
import argparse

import numpy as np
import matplotlib
import matplotlib.pyplot as plt

matplotlib.rcParams['text.usetex'] = True

# Inspiration came from https://stackoverflow.com/q/3609852/8931942


def main(args):
    # Example taken from matplotlib docs
    # https://matplotlib.org/2.0.2/examples/lines_bars_and_markers/fill_demo.html
    x = np.linspace(0, 1, 500)
    y = np.sin(4 * np.pi * x) * np.exp(-5 * x)

    fig, ax = plt.subplots()

    ax.fill(x, y, zorder=10)
    ax.grid(True, zorder=5)

    if hasattr(args, 'plot_title'):
        plt.title(args.plot_title)
    if hasattr(args, 'x_axis_title'):
        plt.xlabel(args.x_axis_title)
    if hasattr(args, 'y_axis_title'):
        plt.ylabel(args.y_axis_title)

    if hasattr(args, 'plot_name'):
        # extensions = ['.pdf', '.png', '.svg', '.jpg']
        # writing SVG is slow
        extensions = ['.pdf', '.png', '.jpg']
        for extension in extensions:
            plt.savefig(args.plot_name + extension)


if __name__ == '__main__':
    cli_parser = argparse.ArgumentParser(
        description='configuration arguments provided at run time from the CLI'
    )
    cli_parser.add_argument(
        '-c',
        '--config_file',
        dest='config_file',
        type=str,
        default=None,
        help='config file',
    )
    cli_parser.add_argument(
        '-x',
        '--x_axis_title',
        dest='x_axis_title',
        type=str,
        default='',
        help='x-axis name',
    )

    args, unknown = cli_parser.parse_known_args()

    parser = argparse.ArgumentParser(parents=[cli_parser], add_help=False)

    if args.config_file is not None:
        if '.json' in args.config_file:
            # The escaping of "\t" in the config file is necesarry as
            # otherwise Python will try to treat is as the string escape
            # sequence for ASCII Horizontal Tab when it encounters it
            # during json.load
            config = json.load(open(args.config_file))
            parser.set_defaults(**config)

            [
                parser.add_argument(arg)
                for arg in [arg for arg in unknown if arg.startswith('--')]
                if arg.split('--')[-1] in config
            ]

    args = parser.parse_args()

    main(args)


At what point should we decide the size of the full input window?  There will just be
one window - it will be fed to the encoder, which will produce a down-sampled set of
local conditioning vectors.  These will be upsampled at the same rate, but without padding.
So, we will then have a smaller set of local conditioning vectors for use with WaveNet.

It's probably best to write Jitter so that it just mimicks its input length on-the-fly.
That's one less calculation to get wrong.  kj

Okay, so we need to calculate the receptive field offsets for each module.  The
only complicated thing seems to be the upsampling.  But, the key insight here
is that the receptive field offsets of the upsampling module are indeed
constant, but the actual source of data needed (the ones not filled in with
zeros) occur only every 320 time steps.  When you then look at their collective
receptive field, you get a window that moves in jumps of 320.  But, it is still
easy to calculate, because each stage of the calculation involves a fixed offset,
followed by an upper bound mod at the stride provided.  I'm still not quite sure
what pattern this will eventually lead to, but it is easily calculable through
an iterative function.  The final challenge will be to make the function
unobtrusive, but deployed at each layer somehow so that the inputs are not somehow
duplicated elsewhere.


New question: why do we really need padding?  we don't actually need to use padding
either.  We just need to make sure that the input is suitably trimmed before it is
applied as a residual connection. 

So, is there any reason to use padded convolutions?  I don't think so - there is no
real merit to it.  So, 


When do we need these offset values?  Actually, we want something slightly different,
which is the reverse of the function we have.  What are the left and right offsets
induced for a particular filter size

The overall calculation needed requires:

1. main_stride
2. sub_stride 
3. filter_sz

The actual left and right offsets will be:

total_off = filter_sz - 1
loff = total_off // 2
roff = total_off - loff
# loff and roff are in units of sub_stride
# we now need to truncate them to the nearest node in the main stride

assert main_stride % sub_stride == 0
stride_ratio = main_stride // sub_stride

# round loff to the nearest main_stride
rem = stride_ratio - (loff % stride_ratio)
loff += rem

# round roff down to the nearest main_stride
rem = roff % stride_ratio
roff -= rem 

loff = loff_tmp * main_stride
roff = roff_tmp * main_stride

# Now, how do we know the main and sub-strides?  We are only
# given the ratio actually.  

The steps involved could also be:

1. calculate filter left and right offsets
2. shrink both offsets to nearest main positions
3. multiply by next dilation factor

The problem with this is that the offset at the first level must be multiplied
by three successive dilation factors.  The offsets at the second level by two
factors.  This seems complicated.

It might be better to instead pre-calculate total strides in terms of the 
final coordinate system.  For a system of strides [5, 4, 4, 4], we have a total
factor of 5 * 4 * 4 * 4 = 320.

This is sort of what we need:

layer strides  main sub  filter_sz  loff    roff   loff_rnd  roff_rnd
4     4          4   1   16         7*1     8*1
3     4         16   4   16         7*4     8*4
2     4         64  16   16         7*16    8*16
1     5        320  64   25         12*64   12*64  


From the convolution / transpose convolution experiment I did a few months ago,
I discovered that 

main(l) = prod_i{l..n}{strides[i]}
sub(l) = prod_i{l+1..n}{strides[i]}
loff(l) = (filter_sz[l] - 1) // 2 * sub(l)
roff(l) = (filter_sz[l] - 1) //+ 2 * sub(l)

So, unfortunately, it turns out that the transpose convolution does generate
output for partial filter overlaps, and this is not desirable.

In keeping with the principle of always using complete convolutions, we will need
to trim the output of transpose convolutions, since unfortunately, there is no
other way to prevent the incomplete ones on the ends.  But, at least this allows
for a consistent left offset.  And, for the right offset, we really just want the
maximal offset, because that guarantees that we have sufficient output.

If we do this, we get a set of offsets such that we can guarantee the desired final
output size.

Where should we specify n_win?  On the one hand, it would be nice to keep it completely
agnostic, and just calculate it from the given window.  But, we do need to know

Now we need a closure for generating the next loss.  This should manage fetching
the next batch of data, running the full model, and computing the loss using the
shifted input.

What module should it be?  Probably it should be just another function
in the autoencoder module.  That way, the logic of calculating the offset
for the encoder input, decoder input, and loss input is local with the
module, as it should be.

Again, we need both the model and data classes to be able to save/restore from
checkpoint.
 

In the data module, we separate the saving and restoring of a checkpoint state
in two stages.  File <-> Encapsulated Checkpoint state <-> Class state

We need to tell the data module the size of windows to produce, and 


         1         2         3
1234567890123456789012345678901234567890
*----*----*----*----*
*********************




What do we want?  We would like only "valid" outputs of the transpose convolution,
which means simply that all of 


It appears that what is happening is the following:

1. pytorch inserts (stride-1) zero elements between each pair of input elements.
So, if there are 5 input elements, the zero-padded input will be 5 + (4 * 4) = 21

2. the filter element is applied in every possible position that overlaps the
input by at least one element, unless padding is non-zero.

With zero padding, this consists of 21 positions in which the key filter
element is positioned atop one of the input positions, plus (21-1)//2 = 12
outputs in which the left wing of the filter is positioned on the first input
element, plus (21-1)//2 = 12 outputs in which the right wing of the filter is
positioned on the last input element.

for each quantity of padding = 1, 2, etc, another of these positions where the key
element is not within the input is eliminated.  So,  


When, during the flow of data, should you convert from numpy ndarrays to torch tensors,
move them to GPU

Obviously, they need to be tensors at or before they must be on GPU.  Also, it seems
cleaner to convert them all at the same time.  The only sticking point is that
there are certain functions that don't exist for torch tensors that still must be
done, like in the preprocessing stages.

The problem currently is that the full autoencoder model includes a pre-processing
step that requires first processing

Where to convert real IDs to consecutive integers?  The data module is in the best
position to do it.  But, the model is where the logic really happens.  


It's the decoder that needs the voice ids.  Where are we going to actually report
these?  From our point of view, the model also is "learning" a mapping between
the voice ids and consecutive integers, because it is in the model parameters that
the integers are actually manifest.  So, it seems appropriate from a design
point of view to store the map in the model.

The problem is, the input offsets are controlled both by this module and others.

Or, is that true?




How is the encoder functioning?  


How really does this rfield offset mechanism work?

The idea is that you have a stack of window-based computations.  These can be
convolutions, transpose convolutions, FFT type functions.  In this case, there are
all three.

We are interested in computing the offsets from the bounding input elements and
bounding output elements.  The output is computed by scanning a window across the input.
For a single scan position, the output element is placed in the center of the input window.
This means that the full output will have an offset at both the left and right.

The rfield offsets for a single window-based module reports the left and right offsets
between its input and output, as well as a field spacing.  

Basically, what we need to know is, for a desired output window size, how much input
do we need?

The rfield.Fieldoffset member answers that question, for an output window size of 1,
this is what you need.

Should this be transitive?  Well, at least in the case of regular convolutions in which
the stride increases, it should be okay.

But, how are we counting the 'size' of the output?  The rfield structure has the notion
of a 





Now, need to encapsulate the loading and saving of the whole thing.  This could
be done in a class that brings together the model and data. 


The basic problem is that I would like a one-stage initialization of modules,
and for shape inference and device inference to happen.  The problem is that
this breaks if

What do we need?

1. member variables quant_pred_snip and wav_compand_out_snip
2. method to update them
3. method to return the loss
4. method to return some other "loss" or metric

Snippet from lb-wavenet:
diffs = tf.argmax(shift_input, 2, output_type=tf.int32) \
        - tf.argmax(logits_out_clip, 2, output_type=tf.int32)
avg_diff = tf.reduce_mean(tf.abs(diffs * use_mask))
 


Do we need multiple random states for the different generators?


The purpose of the _initialize() function is to consolidate all redundant
code that should be executed both in __init__ and __setstate__.

_initialize() doesn't really need to be a zero-argument function.


First thing we need to sort out is, what information is needed to store the state
of the data module?

what uses a random state?
VirtualPermutation
_wav_gen_fn
_load_wav_buffer


_wav_gen 

_wav_gen is a single-pass random permutation of all wav files.  a new wav_gen
is created during _load_wav_buffer when it runs out.

_slice_gen_fn first calls _load_wav_buffer, and then yields a set fraction of
slices from it.

When we pickle a WavSlices instance, we will redo the last call of _load_wav_buffer,
so, the random seed used when it is called is remembered in the variable 


What happens in __init__?

the four member variables are set from the arguments (or derived information),
while another 11 are set to particular values.

In __setstate__, we have a different set of values that need to be set.  However,
we first want to execute the same code as init.


The AE bottleneck simply does an affine transformation of the encoder input
down from 768 to 64 dimensions.  On the other hand, the VAE first does an
affine transform down from 768 dimensions to 128 dimensions.  These are then
split into two groups of 64 - the (mu, sigma) parameters for a 64-dimensional
multivariate gaussian.

We now need a generator for the epsilon variable from the VAE Kingma paper.
These are the samples from the standard multivariate normal.  kj

What is the difference between:

event_shape:  "Shape of a single sample" (from torch.distributions.distribution.py)
batch_shape:  "Shape over which parameters are batched"
sample_shape:  "Used as an argument in sample() and rsample().  The docs say:

"Generates a sample_shape shaped sample"

So, it seems to me that what is happening is the following:

At the simplest level, a multivariate distribution has some number (N) of
dimensions, and a single "sample" from this distribution will thus have this
number of dimensions.  A "sample" could also be called an "event".  Now, although
mathematicians don't really think of the N dimensions as having a "shape", torch
allows you to specify this for some reason.  I'm not sure what that really means though.


The output of the encoder is of shape (N, T, 2*C), with N x T "samples" in a batch, and
each sample has 2*C channels, which we can think of as a (mu, sigma) pair.

Now, with the reparametrization trick, we generate a sample epsilon of shape (C), which
is a sample from a C-dimensional standard multivariate normal N(0, I).

Then, we do an affine transform on epsilon.  The odd thing about this is, I'm
used to thinking of a linear transformation as a layer in which the
*parameters* of the model provide the transformation matrix and the bias
vector.  And, in this case, the activations (I), which are batched (N, T) are
transformed all by the same transformation matrix and bias vector.

But, in this case, it is the activations which are providing the transformation
matrix and bias vector.  And, in a batch (N, T), each of these is different.
So, the gradients will flow through this in "quite a different way".

So, normally, we would have a gradient with respect to the activations, and as
it traveled backwards through the affine transformation, there would be a
gradient w.r.t. the transformation matrix and bias vector.  Additionally, the gradient
also flows past the previous activation, and the process is repeated.

act -> 
        -> act ->
par ->

In this case, we have a gradient w.r.t. the samples from the bottleneck.  These
flow back, producing a gradient w.r.t. the mu, sigma of the samples, which are
the activations.  But, this is exactly what we need.  They won't *update* the
parameters, because these are *not* parameters (they are activations);
nonetheless, the parameters affecting these activations will use this gradient,
so there is no difficulty here.

What is also a bit confusing though, is that, ordinarily, the idiom in PyTorch
is to construct the module during the construction phase, and call its
'forward' and 'backward' methods during the inference and/or training phase.
In this case, the module in question is torch.distributions.normal.  It *does*
have a facility for doing 'reparameterized sampling' in its rsample() method.
Unfortunately, though, this method only accepts the location and scale values as
parameters during module construction time.

The code (from torch.distributions.normal.py):
 
def rsample(self, sample_shape=torch.Size()):
    shape = self._extended_shape(sample_shape)
    eps = _standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
    return self.loc + eps * self.scale
 
What we really want is a forward() method that accepts the 


Implementation of VAE

First off, we need to distinguish between the bottleneck module and the training objective.
The bottleneck module simply tacks on to the encoder output, and makes it stochastic.
It adds:

affine transformation that reduces dimensions from 768 to 128
random sampling from the implied gaussian

Since it is implemented as a module, and the reparameterization trick is used,
the gradient should propagate through it just fine.


Separately, the objective function we use is the SGVB estimator.  It is a loss
function, and we may have to implement it as a subclass of module and write a
custom 'backward' method.

How do we deal with the case where L > 1? (L is the number of encoder samples
per datapoint, mentioned just after equation 8 in the Kingma Welling paper)

For training, the full autoencoder needs to keep track of which dimension corresponds
with L.  The easiest thing would be to augment B.

The way it would work is as follows:

In the following, here are the dimension abbreviations:
B: n_batch
T1: n_timesteps
C: n_input_channels
T2: n_encoding_timesteps
E: n_encoder_channels
K: n_bottleneck_channels
L: n_vae_samples_per_datapoint



A batch of overlapping input waveform windows of shape (B, T1, C) would be fed
in to the encoder, which outputs vectors of shape (B, T2, E).  These are fed
into the bottleneck.  The VAE bottleneck could output (B * L, T2, K) or (B, L,
T2, K).  However, the decoder is designed to accept conditioning shapes of
wav_onehot: (B, I, T1) and lc_sparse: (B, L, T2)

Should we engineer WaveNet to accept another channel?  Well, in inference mode,
the individual Waveforms would all diverge, even if the conditioning vectors
came from the same inputs.

In teacher-forcing (training) mode, it is mildly wasteful, since we are
duplicating the teacher L times.  However, there really isn't a choice.  Those
values need to be broadcast at some point anyway, whether internally by the
WaveNet module, or externally, and then fed into WaveNet.

It seems like, in order to avoid a code change to WaveNet itself, which would
be irrelevant for inference mode, the best choice is to duplicate the
wav_onehot input L times, along the B dimension.

Now, having difficulties with util.gather_md.  At first, I had thought that there was
only one sensible design of such a "multidimensional gather", but there are a couple:

Both designs interpret the element values in 'index' as index values of the 'dim' dimension
in 'input'.  Since the 'dim' dimension of 'input' may have some size S, this implies that
each 'select' operation retrieves a single cell among the S possible cells.

However, the different design choices involve how one interprets the way the choices of
all of the other indices (in the input and the query) are combined together.

This basically boils down to a SQL query with a join condition.  I use the
shorthand notation: (1..k / d) meaning "all values from 1 to k excluding d":


d: integer in (1..k)
input: i_(1..k), ival
query: q_(1..m), qval

SELECT i_(1..k / d), q_(1..m), ival
from input, query
where i_d = qval


d: integer in (1..k)
Input: i_(1..k), ival
Query: q_(1..k / d), qval
NOTE: max(q_l) == max(i_l) for all l in (1..k / d)

SELECT i_(1..k / d), ival
from input, query
where i_d = qval 
and i_l = q_l for l in (1..k / d)


What torch.index_select does, expressed as SQL:

d: integer in (1..k)
input: i_(1..k), ival
query: q_1, qval

SELECT i_1, i_2, ..., i_(d-1), q_1, i_(d_1), ..., i_k, ival
from input, query
where i_d = qval

